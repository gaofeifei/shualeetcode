每天一题，时间控制在0.5小时,已经安装好插件
第一题 https://leetcode.com/problems/two-sum
体会:
第二题 难，不会

--
机器学习：
	1 写出全概率公式&贝叶斯公式
	2 模型训练为什么要引入偏差bias和方差variance?证
	3 CRF/朴树贝叶斯、EM、最大熵模型、马尔科夫随机场、混合高斯模型
	4 如何解决过拟合问题
	5 ONE-HOT的作用是什么，为啥不直接使用数字作为表示？
	6 决策树和随机森林的区别是什么
	7 朴素贝叶斯为什么"朴素naive"
	8 kmaens促使点除了随机选取之外的方法
	9 LR明明是分类模型为什么叫回归
	10 梯度下降如何并行化
	11 LR中的L1/L2正则项是啥
	12 简述决策树构建过程
	13 解释Gini系数
	14 决策树的优缺点
	15 出估现计概率值为0怎么处理
	16 随机森林的生成过程
	17 介绍一下Boosting的思想
	18 gbdt的中的tree是什么tree,有什么特征
	19 xgboost对比gbdt/boostingTree有了那些方向上的优化
	20 什么叫最超优平面
	21 什么是支持向量
	22 SVM如何解决多分类问题
	23 核函数的作用是啥
特征工程：
	1 怎么去除dataFrame里的缺失值
	2 特征无量纲化的常见操作方法
	3 如何对类别变量进行独热编码
	4 如何把年龄字段按照我们的阈值分段
	5 如何根据变量相关性画出热力图
	6 如何把分布修正为类正态分布
	7 怎么简单使用PCA来划分数据且可视化
	8 怎么简单使用LDA来划分数据且可视化
深度学习：
	1 你觉得batch-normalization过程是什么样的
	2 激活函数有什么用，常见的激活函数的区别是什么
	3 softmax的原理是什么，有什么用，CNN的平移不变性是什么，如何实现
	4 VGG,GoogleNet,ResNet等网络之间的区别是什么
	5 残差网络为什么能解决梯度消失的问题
	6 LSTM为什么能解决梯度消失、爆炸的问题
	7 Attention对比RNN和CNN，分别有那点你觉得的优势
	8 写出Attention机制里面的q,k,v分别代表什么
	9 为什么self-attention可以替代seq2seq
NLP：
	1 Golve的损失函数
	2 为什么Golve会用的相对比W2V少
	3 层次softmax流程
	4 负采样流程
	5 怎么衡量学到的embedding的好坏
	6 阐述CRF原理
	7 详述LDA原理
	8 LDA中的主体矩阵如何计算
	9 LDA和W2V的区别
	10 LDA和Doc2Vec区别
	11 Bert的双向体现在什么地方
	12 Bert是怎样预训练的
	13 在数据中随机选择15%的标记，其中80%被换位mask,10%随机不变，10%速记替换其他单词，原因是什么
	14 为什么BERT有3个嵌入层，他们是如何实现的
	15 手写一个multi-head attention
推荐系统：
	1 DNN与DeepFM之间的区别
	2 你在使用deepFM的时候是如何处理欠拟合和过拟合问题的
	3 deepFM的embedding初始化有什么值得注意的地方
	4 youtubeNet变长数据如何处理
	5 youtubeNet如何避免百万量级的softmax问题的
	6 推荐系统有哪些评测指标
	7 MLR的原理是什么，做了哪些优化
知识图谱: