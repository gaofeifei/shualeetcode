第3章	深度学习基础	

一	基本概念
1	神经网络组成
2	25种神经网络有哪些常用模型结构
3	如何选择深度学习开发平台
4	为什么使用深层表示
5	为什么深层神经网络难以训练
6	深度学习和机器学习有什么不同


二	神经网络操作与计算
1	前向传播与反向传播
2	如何计算神经网络的输出
3	如何计算卷积神经网络输出值
4	如何计算Pooling层输出值输出值
5	实例理解反向传播
6	神经网络更“深”有什么意义



三	激活函数
1	为什么需要激活函数
2	为什么激活函数需要非线性函数
3	常见的激活函数及图像
4	常见激活函数的导数计算
5	激活函数有哪些性质
6	如何选择激活函数
7	使用 ReLu 激活函数的优点
8	什么时候可以用线性激活函数
9	怎样理解Relu(<0时)是非线性激活函数
10	Softmax 定义及作用
11	Softmax 函数如何应用于多分类
12	交叉熵代价函数定义及其求导推导
13	为什么Tanh收敛速度比Sigmoid快
14	内聚外斥-Center Loss


四BatchSize
1	为什么需要Batch_Size
2	Batch_Size 值的选择
3	在合理范围内，增大Batch_Size有何好处
4	盲目增大 Batch_Size 有何坏处
5	调节Batch_Size对训练效果影响到底如何


五归一化
1	归一化含义
2	为什么要归一化
3	为什么归一化能提高求解最优解速度
4	归一化有哪些类型
5	局部响应归一化作用
6	理解局部响应归一化
7	什么是批归一化
8	批归一化（BN）算法的优点
9	批归一化（BN）算法流程
10	批归一化和群组归一化比较
11	WeightNormalization和Batch Normalization比较
12	BatchNormalization在什么时候用比较合适


六参数初始化-权重偏差初始化
1	全都初始化为 0
2	全都初始化为同样的值
3	初始化为小的随机数
4	用校准方差
5	稀疏初始化(SparseInitialazation)
6	初始化偏差


七预训练与微调(fine tuning)
1	为什么无监督预训练可以帮助深度学习
2	什么是模型微调fine tuning
3	微调时候网络参数是否更新
4	fine-tuning 模型的三种状态


八超参数
1	什么是超参数
2	如何寻找超参数的最优值
3	超参数搜索一般过程


九学习率
1	学习率的作用
2	学习率衰减常用参数有哪些
3	分段常数衰减
4	指数衰减
5	自然指数衰减
6	多项式衰减
7	余弦衰减


十正则化-Dropout 系列问题
1	为什么要正则化
2	为什么正则化有利于预防过拟合
3	理解dropout正则化 
4	dropout率的选择
5	dropout有什么缺点
6	深度学习中常用的数据增强方法
7	如何理解InternalCovariate Shift



第2章	机器学习基础

一	基本概念
1	各种常见算法都有哪些
2	理解局部最优与全局最优
3	机器学习4种学习方式
4	监督学习有哪些步骤


二	分类算法
1	常用分类算法的优缺点
2	分类算法的评估方法常用术语
3	分类算法的评估评价指标
4	分类算法的评估ROC曲线和PR曲线
5	正确率能很好的评估分类算法吗
6	什么样的分类器是最好的


三	逻辑回归
1	逻辑回归适用性
2	生成模型和判别模型的区别
3	逻辑回归与朴素贝叶斯有什么区别
4	线性回归与逻辑回归的区别


四	代价函数
1	代价函数作用原理
2	平方误差代价函数的主要思想
3	为什么代价函数要非负
4	常见代价函数
5	为什么不用二次方代价函数
6	为什么要用交叉熵

五	损失函数
1	什么是损失函数
2	逻辑回归为什么使用对数损失函数
3	对数损失函数是如何度量损失的
4	代价函数与损失函数的区别


六	梯度下降
1	机器学习中为什么需要梯度下降
2	梯度下降法缺点
3	梯度下降算法的核心思想归纳
4	梯度下降法算法描述
5	如何对梯度下降法进行调优
6	批量梯度下降的求解思路
7	随机梯度下降的求解思路
8	随机梯度和批量梯度区别
9	小批量（Mini-Batch）梯度下降的求解思路
10	各种梯度下降法性能比较
11	自然梯度法
12	Fisher信息矩阵的意义


七	线性判别分析LDA
1	LDA思想总结
2	二类LDA算法原理
3	LDA算法流程总结
4	LDA和PCA区别
5	LDA优缺点


八	主成分分析PCA
1	主成分分析（PCA）思想总结
2	PCA算法流程总结
3	PCA算法主要优缺点
4	降维的必要性及目的
5	KPCA与PCA的区别


九	决策树
决策树是一个预测模型，它代表的是对象属性和对象值之间的一种映射关系
1	决策树的基本原理
	决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。
2	决策树的三要素
	(1)、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。 

​	(2)、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。 

​	(3)、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。
3	决策树学习基本算法P62伪代码总结
	(1)
	(2)
	(3)
	(4)
	(5)
4	决策树算法优缺点
	优点：
	(1)、决策树算法易理解，机理解释起来简单。 
	(2)、决策树算法可以用于小数据集。
	(3)、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。
	(4)、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。
	(5)、能够处理多输出的问题。 
	(6)、对缺失值不敏感。
	(7)、可以处理不相关特征数据。
	(8)、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
	缺点：
	(1)、对连续性的字段比较难预测。
	(2)、容易出现过拟合。
	(3)、当类别太多时，错误可能就会增加的比较快。
	(4)、在处理特征关联性比较强的数据时表现得不是太好。
	(5)、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。

5	熵的概念以及理解
	熵：度量随机变量的不确定性。

6	信息增益的理解
	以某特征划分数据集前后的熵的差值g(D,A)=H(D)-H(D|A)。计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。
	信息增益比：信息增益比=惩罚参数*信息增益
	信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。  
​	惩罚参数：数据集D以特征A作为随机变量的熵的倒数。

7	决策树中熵，条件熵，信息增益的联系
	熵：表示一个随机变量的复杂性或不确定性。
	条件熵:表示在知道某一条件后某一随机变量的复杂性和不确定性。
	信息增益:道某一条件后某一随机变量的不确定性的减少量。书P65买衣服例子。

8	剪枝处理的作用及策略
	用来解决过拟合问题的一种办法。
	在决策树算法中，为了尽可能正确分类训练样本，节点划分过程不断重复，有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点，而导致过拟合。因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。 
​	剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。
​	预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。 
​	后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。

十	支持向量机
1	什么是支持向量机
2	支持向量机能解决哪些问题
3	核函数特点及其作用
4	SVM为什么引入对偶问题
5	如何理解SVM中的对偶问题
6	常见的核函数有哪些
7	SVM主要特点
8	SVM主要缺点
9	逻辑回归与SVM的异同

十一	贝叶斯分类器
1	极大似然估计原理
2	贝叶斯分类器基本原理
3	朴素贝叶斯分类器
4	半朴素贝叶斯分类器
5	极大似然估计和贝叶斯估计的联系与区别

十二	EM算法
1	EM算法基本思想
2	EM算法流程

十三	降维和聚类
1	为什么会产生维数灾难
2	怎样避免维数灾难
3	聚类和降维有什么区别与联系
4	有哪些聚类算法优劣衡量标准
5	聚类和分类有什么区别
6	四种常用聚类方法之比较


第1章 数学基础
[数学概念]
	标量
	向量
	矩阵
	张量
	导数
	偏导数
	特征值
	特征向量
	概率分布
	随机变量
	离散型随机变量和概率质量函数
	连续型随机变量和概率密度函数
	条件概率
	独立性
	条件独立性
	期望
	方差
	协方差
	相关系数

[问]如何判断一个矩阵为正定
[问]导数和偏导数有什么区别
[问]奇异值与特征值有什么关系
[问]机器学习为什么要使用概率
[问]变量与随机变量有什么区别
[问]随机变量与概率分布的联系
[问]联合概率与边缘概率联系区别  
[问]条件概率的链式法则
[问]常见概率分布
[问]何时采用正态分布


1.6 期望、方差、协方差、相关系数
1期望举例：

2方差举例：

3协方差举例：

4相关系数举例：


