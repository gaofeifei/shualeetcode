第3章	深度学习基础	

一	基本概念
1	神经网络组成
2	25种神经网络有哪些常用模型结构
3	如何选择深度学习开发平台
4	为什么使用深层表示
5	为什么深层神经网络难以训练
6	深度学习和机器学习有什么不同


二	神经网络操作与计算
1	前向传播与反向传播
2	如何计算神经网络的输出
3	如何计算卷积神经网络输出值
4	如何计算Pooling层输出值输出值
5	实例理解反向传播
6	神经网络更“深”有什么意义



三	激活函数
1	为什么需要激活函数
2	为什么激活函数需要非线性函数
3	常见的激活函数及图像
4	常见激活函数的导数计算
5	激活函数有哪些性质
6	如何选择激活函数
7	使用 ReLu 激活函数的优点
8	什么时候可以用线性激活函数
9	怎样理解Relu(<0时)是非线性激活函数
10	Softmax 定义及作用
11	Softmax 函数如何应用于多分类
12	交叉熵代价函数定义及其求导推导
13	为什么Tanh收敛速度比Sigmoid快
14	内聚外斥-Center Loss


四BatchSize
1	为什么需要Batch_Size
2	Batch_Size 值的选择
3	在合理范围内，增大Batch_Size有何好处
4	盲目增大 Batch_Size 有何坏处
5	调节Batch_Size对训练效果影响到底如何


五归一化
1	归一化含义
2	为什么要归一化
3	为什么归一化能提高求解最优解速度
4	归一化有哪些类型
5	局部响应归一化作用
6	理解局部响应归一化
7	什么是批归一化
8	批归一化（BN）算法的优点
9	批归一化（BN）算法流程
10	批归一化和群组归一化比较
11	WeightNormalization和Batch Normalization比较
12	BatchNormalization在什么时候用比较合适


六参数初始化-权重偏差初始化
1	全都初始化为 0
2	全都初始化为同样的值
3	初始化为小的随机数
4	用校准方差
5	稀疏初始化(SparseInitialazation)
6	初始化偏差


七预训练与微调(fine tuning)
1	为什么无监督预训练可以帮助深度学习
2	什么是模型微调fine tuning
3	微调时候网络参数是否更新
4	fine-tuning 模型的三种状态


八超参数
1	什么是超参数
2	如何寻找超参数的最优值
3	超参数搜索一般过程


九学习率
1	学习率的作用
2	学习率衰减常用参数有哪些
3	分段常数衰减
4	指数衰减
5	自然指数衰减
6	多项式衰减
7	余弦衰减


十正则化-Dropout 系列问题
1	为什么要正则化
2	为什么正则化有利于预防过拟合
3	理解dropout正则化 
4	dropout率的选择
5	dropout有什么缺点
6	深度学习中常用的数据增强方法
7	如何理解InternalCovariate Shift



第2章	机器学习基础

一	基本概念
1	各种常见算法都有哪些
2	理解局部最优与全局最优
3	机器学习4种学习方式
4	监督学习有哪些步骤


二	分类算法
1	常用分类算法的优缺点
2	分类算法的评估方法常用术语
3	分类算法的评估评价指标
4	分类算法的评估ROC曲线和PR曲线
5	正确率能很好的评估分类算法吗
6	什么样的分类器是最好的


三	逻辑回归
1	逻辑回归适用性
	（1）用于概率预测。
	（2）用于分类。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。
	（3）寻找危险因素。
	（4）仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。
	（5）各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。

2	生成模型和判别模型的区别
	(1)生成模型：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)（贝叶斯概率）。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。典型的生成模型有朴素贝叶斯，隐马尔科夫模型等。
	(2)判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。这些模型的特点都是输入属性X可以直接得到后验概率P(Y|X)，输出条件概率最大的作为最终的类别（对于二分类任务来说，实际得到一个score，当score大于threshold时则为正类，否则为负类）。
	(3)联系和区别：
 		生成方法的特点：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y,X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。
 		判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。
​		最后，由生成模型可以得到判别模型，但由判别模型得不到生成模型。

3	逻辑回归与朴素贝叶斯有什么区别
（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。
（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然估计。
（3）朴素贝叶斯需要条件独立假设。
（4）逻辑回归需要求特征参数间是线性的。

4	线性回归与逻辑回归的区别
线性回归:预测；未知；拟合函数；参数计算方式是最小二乘法；连续值。
逻辑回归:分类；（0,1）；预测函数；参数计算方式是极大似然估计；离散值。

四	代价函数 cost function所有样本误差的平均即损失函数的平均。fit curve拟合曲线，raw data原始数据
为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。用来找到最优解的目的函数。
1	代价函数作用原理
	在回归问题中，通过代价函数来求解最优解，常用的是平方误差代价函数。

2	平方误差代价函数的主要思想：将实际数据给出的值与拟合出的线的对应值做差，求出拟合出的直线与实际的差距。在实际应用中，为了避免因个别极端数据产生的影响，采用类似方差再取二分之一的方式来减小个别数据的影响。最优解即为代价函数的最小值。

3	为什么代价函数要非负
	目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。

4	常见代价函数 J
	（1）二次代价函数（quadratic cost）在使用sigmoid函数的情况下, 初始的代价（误差）越大，导致训练越慢。
	（2）交叉熵代价函数（cross-entropy）
	二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是S型函数的情况。
	（3）对数似然代价函数（log-likelihood cost）对数似然函数常用来作为softmax回归的代价函数。深度学习中普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然代价函数。
	对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。
	我们将似然函数作为机器学习模型的损失函数，并且用在分类问题中。这时似然函数是直接作用于模型的输出的（损失函数就是为了衡量当前参数下model的预测值predict距离真实值label的大小，所以似然函数用作损失函数时当然也是为了完成该任务），所以对于似然函数来说，这里的样本集就成了label集（而不是机器学习意义上的样本集X了），这里的参数也不是机器学习model 的参数，而是predict值。
	其实作为损失函数的似然函数并不关心你当前的机器学习model的参数是怎样的，毕竟它此时所接收的输入只有两部分：1、predict。2、label 。3、分布模型（predict服从的分布）。
	显然这里的label就是似然函数的观测值，即样本集。而它眼里的模型，当然就是predict这个随机变量所服从的概率分布模型。它的目的，就是衡量predict背后的模型对于当前观测值的解释程度。而每个样本的predict值，恰恰就是它所服从的分布模型的参数。
	比如此时我们的机器学习任务是一个4个类别的分类任务，机器学习model的输出就是当前样本X下的每个类别的概率，如predict=[0.1, 0.1, 0.7, 0.1]，而该样本的标签是类别3，表示成向量就是label=[0, 0, 1, 0]。那么label=[0, 0, 1, 0]就是似然函数眼里的样本，然后我们可以假设predict这个随机变量背后的模型是单次观测下的多项式分布，（因为softmax本身是基于多项式分布的）。







5	为什么不用二次方代价函数

6	为什么要用交叉熵

五	损失函数
1	什么是损失函数
2	逻辑回归为什么使用对数损失函数
3	对数损失函数是如何度量损失的
4	代价函数与损失函数的区别


六	梯度下降
1	机器学习中为什么需要梯度下降
2	梯度下降法缺点
3	梯度下降算法的核心思想归纳
4	梯度下降法算法描述
5	如何对梯度下降法进行调优
6	批量梯度下降的求解思路
7	随机梯度下降的求解思路
8	随机梯度和批量梯度区别
9	小批量（Mini-Batch）梯度下降的求解思路
10	各种梯度下降法性能比较
11	自然梯度法
12	Fisher信息矩阵的意义


七	线性判别分析LDA
1	LDA思想总结
2	二类LDA算法原理
3	LDA算法流程总结
4	LDA和PCA区别
5	LDA优缺点


八	主成分分析PCA
1	主成分分析（PCA）思想总结
2	PCA算法流程总结
3	PCA算法主要优缺点
4	降维的必要性及目的
5	KPCA与PCA的区别


九	决策树
决策树是一个预测模型，它代表的是对象属性和对象值之间的一种映射关系
1	决策树的基本原理
	决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。
2	决策树的三要素
	(1)、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。 

​	(2)、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。 

​	(3)、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。
3	决策树学习基本算法P62伪代码总结
	(1)
	(2)
	(3)
	(4)
	(5)
4	决策树算法优缺点
	优点：
	(1)、决策树算法易理解，机理解释起来简单。 
	(2)、决策树算法可以用于小数据集。
	(3)、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。
	(4)、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。
	(5)、能够处理多输出的问题。 
	(6)、对缺失值不敏感。
	(7)、可以处理不相关特征数据。
	(8)、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
	缺点：
	(1)、对连续性的字段比较难预测。
	(2)、容易出现过拟合。
	(3)、当类别太多时，错误可能就会增加的比较快。
	(4)、在处理特征关联性比较强的数据时表现得不是太好。
	(5)、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。

5	熵的概念以及理解
	熵：度量随机变量的不确定性。

6	信息增益的理解
	以某特征划分数据集前后的熵的差值g(D,A)=H(D)-H(D|A)。计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。
	信息增益比：信息增益比=惩罚参数*信息增益
	信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。  
​	惩罚参数：数据集D以特征A作为随机变量的熵的倒数。

7	决策树中熵，条件熵，信息增益的联系
	熵：表示一个随机变量的复杂性或不确定性。
	条件熵:表示在知道某一条件后某一随机变量的复杂性和不确定性。
	信息增益:道某一条件后某一随机变量的不确定性的减少量。书P65买衣服例子。

8	剪枝处理的作用及策略
	用来解决过拟合问题的一种办法。
	在决策树算法中，为了尽可能正确分类训练样本，节点划分过程不断重复，有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点，而导致过拟合。因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。 
​	剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。
​	预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。 
​	后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。

十	支持向量机 是一种分类算法，解决小样本，非线性及高维模式识别中有特有优势，并能够推广应用函数拟合等机器学习问题中。
1	什么是支持向量机
	支持向量：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。
	支持向量机（SupportVectorMachine，SVM）通过支持向量运算的分类器。
	支持向量机是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是边界最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：
​	(1)当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；
​	(2)当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；
	(3)当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；

2	支持向量机能解决哪些问题
	(1)线性分类
	在训练数据中，每个数据都有n个的属性和一个二分类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面，这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。这样的超平面有很多，假如我们要找到一个最佳的超平面。此时，增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的，成为最大边距超平面。这个分类器即为最大边距分类器。
	(2)非线性分类
	拉格朗日乘子法，在极值点，圆与曲线相切
	https://www.zhihu.com/question/38586401
	SVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件，以及核函数可以生成非线性分类器。

3	核函数特点及其作用
	引入核函数目的：把原坐标系里线性不可分的数据用核函数Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。  
​	核函数方法的广泛应用，与其特点是分不开的：  

	1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。

	2）无需知道非线性变换函数Φ的形式和参数。

	3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。

	4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。

4	SVM为什么引入对偶问题
	对偶问题的性质：
		a对偶问题的对偶是原问题；
	​	b无论原始问题是否是凸的，对偶问题都是凸优化问题；
	​	c对偶问题可以给出原始问题一个下界；
	​	d当满足一定条件时，原始问题与对偶问题的解是完全等价的
	(1)对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。
	(2)可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）。
	(3)在优化理论中，目标函数 f(x) 会有多种形式：如果目标函数和约束条件都为变量 x 的线性函数，称该问题为线性规划；如果目标函数为二次函数，约束条件为线性函数，称该最优化问题为二次规划；如果目标函数或者约束条件均为非线性函数，称该最优化问题为非线性规划。

5	如何理解SVM中的对偶问题
	在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。
	对偶问题的求解步骤
	step 1. 转化问题
	step 2.现在的问题是如何找到问题(模型函数，最小二乘法)的最优值的一个最好的下界? 
	maxminL(w,b,a)

6	常见的核函数有哪些
	LinearKernel线性核
	PolynomialKernel多项式核
	ExponentialKernel指数核
	GaussianKernel高斯核
	LaplacianKernel拉普拉斯核
	ANOVAKernel
	SigmoidKernel

7	SVM主要特点 SVM是一个凸优化问题
	(1)  SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。  
	(2)  SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。  
	(3)  支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。  
	(4)  SVM是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。
	(5)  SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。  
	(6)  少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：
​        ①增、删非支持向量样本对模型没有影响;  
​        ②支持向量样本集具有一定的鲁棒性;  
​        ③有些成功的应用中，SVM方法对核的选取不敏感  
	(7)  SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。  
	(8)  SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。 
	(9)  SVM在小样本训练集上能够得到比其它算法好很多的结果。SVM优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。  
	(10)  它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。  

8	SVM主要缺点
	(1) SVM算法对大规模训练样本难以实施  
​	SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。  
​	如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。
	(2) 用SVM解决多分类问题存在困难
	经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。
	(3) 对缺失数据敏感，对参数和核函数的选择敏感
​	支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。

9	逻辑回归与SVM的异同
相同点：
	LR和SVM都是 分类,监督学习,判别模型,在不考虑核函数时，LR和SVM都是线性分类算法，他们的分类决策面都是线性的。

不同点：
	(1)LR采用log损失，SVM采用合页(hinge)损失。
		逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。  
	​	支持向量机基于几何边界最大化原理，认为存在最大几何边界的分类面为最优分类面。
	(2)LR对异常值敏感，SVM对异常值不敏感。
		支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。  
	​	支持向量机改变非支持向量样本并不会引起决策面的变化。  
	​	逻辑回归中改变任何样本都会引起决策面的变化。
	(3)计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高
	(4)对非线性问题的处理方式不同
		LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM则可以通过对偶求解高效处理。LR则在特征空间维度很高时，表现较差。
	(5)SVM的损失函数就自带正则。  
	​	损失函数中的1/2||w||^2项，这就是为什么SVM是结构风险最小化算法的原因。而LR必须另外在损失函数上添加正则项。
	(6)SVM自带结构风险最小化，LR则是经验风险最小化。
	(7)SVM会用核函数而LR一般不用核函数。

十一	贝叶斯分类器
1	极大似然估计原理
2	贝叶斯分类器基本原理
3	朴素贝叶斯分类器
4	半朴素贝叶斯分类器
5	极大似然估计和贝叶斯估计的联系与区别

十二	EM算法
1	EM算法基本思想
2	EM算法流程

十三	降维和聚类
1	为什么会产生维数灾难
2	怎样避免维数灾难
3	聚类和降维有什么区别与联系
4	有哪些聚类算法优劣衡量标准
5	聚类和分类有什么区别
6	四种常用聚类方法之比较


第1章 数学基础
[数学概念]
	标量
	向量
	矩阵
	张量
	导数
	偏导数
	特征值
	特征向量
	概率分布
	随机变量
	离散型随机变量和概率质量函数
	连续型随机变量和概率密度函数
	条件概率
	独立性
	条件独立性
	期望
	方差
	协方差
	相关系数

[问]如何判断一个矩阵为正定
[问]导数和偏导数有什么区别
[问]奇异值与特征值有什么关系
[问]机器学习为什么要使用概率
[问]变量与随机变量有什么区别
[问]随机变量与概率分布的联系
[问]联合概率与边缘概率联系区别  
[问]条件概率的链式法则
[问]常见概率分布
[问]何时采用正态分布


1.6 期望、方差、协方差、相关系数
1期望举例：

2方差举例：

3协方差举例：

4相关系数举例：


